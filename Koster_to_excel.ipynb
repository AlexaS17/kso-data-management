{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Koster_to_excel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nST_NIefwj5C"
      },
      "source": [
        "# Koster data to excel\n",
        "\n",
        "The following scripts are set up to retrieve the annotations, comments and tags from the Koster seafloor observatory and translate them to excel-friendly format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItN_oYTkwj5D"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMtnwGLswj5D"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjHJ-llOx0eZ"
      },
      "source": [
        "We use the \"panoptes_client\" package to communicate with Zooniverse. If you don't have it installed, run the command below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5sxXdZex2M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea86abd5-d1e9-48a7-fecf-ca7d7c54ac49"
      },
      "source": [
        "!pip install panoptes_client"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting panoptes_client\n",
            "  Downloading https://files.pythonhosted.org/packages/55/6d/09aee478aedcbdc87825eb39bb8593392dc1743b3066d25ba9ec35aa75b0/panoptes_client-1.3.0.tar.gz\n",
            "Requirement already satisfied: requests<2.25,>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from panoptes_client) (2.23.0)\n",
            "Requirement already satisfied: future<0.19,>=0.16 in /usr/local/lib/python3.6/dist-packages (from panoptes_client) (0.16.0)\n",
            "Collecting python-magic<0.5,>=0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/59/77/c76dc35249df428ce2c38a3196e2b2e8f9d2f847a8ca1d4d7a3973c28601/python_magic-0.4.18-py2.py3-none-any.whl\n",
            "Collecting redo>=1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/df/6eaeece84b3b6a51663075ae25089ec9b49e90b687ddca6f1fe0f93ab091/redo-2.0.4.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from panoptes_client) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.25,>=2.4.2->panoptes_client) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.25,>=2.4.2->panoptes_client) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.25,>=2.4.2->panoptes_client) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.25,>=2.4.2->panoptes_client) (2.10)\n",
            "Building wheels for collected packages: redo\n",
            "  Building wheel for redo (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for redo: filename=redo-2.0.4-cp36-none-any.whl size=11931 sha256=85c19a343973632767be3c204c02b06092e1887ac85976452b2ec6e647c6b2c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/ca/39/576f2d470fab4725bb098ca3a1889ee540875e2bd072dc7ec0\n",
            "Successfully built redo\n",
            "Building wheels for collected packages: panoptes-client\n",
            "  Building wheel for panoptes-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for panoptes-client: filename=panoptes_client-1.3.0-cp36-none-any.whl size=30665 sha256=6841a0b45e3720fb096fe47a86f0ce1b0b4e794396358ca525757095200ce930\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/47/7f/52807dc212183f9e45497a2071717d85bef5658b9481fc4074\n",
            "Successfully built panoptes-client\n",
            "Installing collected packages: python-magic, redo, panoptes-client\n",
            "Successfully installed panoptes-client-1.3.0 python-magic-0.4.18 redo-2.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygHmKZGlx8vb"
      },
      "source": [
        "### Load required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRLefFV2yAfO"
      },
      "source": [
        "import io\n",
        "import zipfile\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "from datetime import date\n",
        "from panoptes_client import (\n",
        "    SubjectSet,\n",
        "    Subject,\n",
        "    Project,\n",
        "    Panoptes,\n",
        ") "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKYQ2v_yEp4"
      },
      "source": [
        "### Connect to Zooniverse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh356o6nyO6r"
      },
      "source": [
        "zoo_user = \"user\"\n",
        "zoo_pass = \"pass\"\n",
        "\n",
        "# Connect to Zooniverse with your username and password\n",
        "auth = Panoptes.connect(username=zoo_user, password=zoo_pass)\n",
        "\n",
        "if not auth.logged_in:\n",
        "    raise AuthenticationError(\"Your credentials are invalid. Please try again.\")\n",
        "\n",
        "# Connect to the Zooniverse project (our project # is 9747)\n",
        "project = Project(9747)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDoVfs0XzfVr"
      },
      "source": [
        "# Download Zooniverse subjects information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doHKpJHiwj5E"
      },
      "source": [
        "# Get info of subjects uploaded to the project\n",
        "export = project.get_export(\"subjects\")\n",
        "\n",
        "# Save the subjects info as pandas data frame\n",
        "subjects_df = pd.read_csv(\n",
        "    io.StringIO(export.content.decode(\"utf-8\")),\n",
        "    usecols=[\n",
        "        \"subject_id\",\n",
        "        \"metadata\",\n",
        "        \"created_at\",\n",
        "        \"workflow_id\",\n",
        "        \"subject_set_id\",\n",
        "        \"classifications_count\",\n",
        "        \"retired_at\",\n",
        "        \"retirement_reason\",\n",
        "    ],\n",
        ")"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HqluuLa0t8G"
      },
      "source": [
        "## Format subject information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "injQq7YE1lNe"
      },
      "source": [
        "### Define project-specific functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZUATgrY2gKA"
      },
      "source": [
        "Function to extract the metadata from subjects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqW5Pd5A1t0g"
      },
      "source": [
        "def extract_metadata(subj_df):\n",
        "\n",
        "    # Reset index of df\n",
        "    subj_df = subj_df.reset_index(drop=True).reset_index()\n",
        "\n",
        "    # Flatten the metadata information\n",
        "    meta_df = pd.json_normalize(subj_df.metadata.apply(json.loads))\n",
        "\n",
        "    # Drop metadata and index columns from original df\n",
        "    subj_df = subj_df.drop(columns=[\"metadata\", \"index\",])\n",
        "\n",
        "    return subj_df, meta_df"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sme3HTFu1MHe"
      },
      "source": [
        "### Format subjects uploaded automatically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0a9163_2E6f"
      },
      "source": [
        "# Specify the date when we first started uploading subjects automatically\n",
        "first_auto_upload = \"2020-05-29 00:00:00 UTC\"\n",
        "\n",
        "# Select automatically uploaded frames\n",
        "auto_subjects_df = subjects_df[subjects_df[\"created_at\"] > first_auto_upload]\n",
        "\n",
        "# Extract metadata from automatically uploaded frames\n",
        "auto_subjects_df, auto_subjects_meta = extract_metadata(auto_subjects_df)\n",
        "\n",
        "# Combine metadata info with the subjects df\n",
        "auto_subjects_df = pd.concat([auto_subjects_df, auto_subjects_meta], axis=1)\n",
        "\n",
        "# Select only relevant columns\n",
        "auto_subjects_df = auto_subjects_df[\n",
        "    [\"subject_id\", \"retired_at\", \"subject_type\"]\n",
        "]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLsq1BVJ2CMW"
      },
      "source": [
        "### Format subjects uploaded manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUX56zIE00E2"
      },
      "source": [
        "# Specify the starting date when clips were manually uploaded\n",
        "first_manual_upload = \"2019-11-17 00:00:00 UTC\"\n",
        "\n",
        "# Select subjects uploaded manually\n",
        "man_clips_df = (\n",
        "    subjects_df[\n",
        "        (subjects_df[\"metadata\"].str.contains(\".mp4\"))\n",
        "        & (\n",
        "            subjects_df[\"created_at\"].between(\n",
        "                first_manual_upload, first_auto_upload\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        "    .reset_index(drop=True)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Specify the type of subject\n",
        "man_clips_df[\"subject_type\"] = \"clip\"\n",
        "\n",
        "# Extract metadata from manually uploaded clips\n",
        "man_clips_df, man_clips_meta = extract_metadata(man_clips_df)\n",
        "\n",
        "# Combine metadata info with the subjects df\n",
        "man_clips_df = pd.concat([man_clips_df, man_clips_meta], axis=1)\n",
        "\n",
        "# Select only relevant columns\n",
        "man_clips_df = man_clips_df[\n",
        "    [\"subject_id\", \"retired_at\", \"subject_type\"]\n",
        "]\n",
        "\n",
        "# Combine all uploaded subjects\n",
        "subjects = pd.merge(man_clips_df, auto_subjects_df, how=\"outer\")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkRr7l4FPIVs"
      },
      "source": [
        "# Download Zooniverse classifications information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JknNX3ZpPVFq"
      },
      "source": [
        "# Get classifications from Zooniverse\n",
        "export = project.get_export(\"classifications\")\n",
        "\n",
        "# Save the response as pandas data frame\n",
        "class_df = pd.read_csv(\n",
        "    io.StringIO(export.content.decode(\"utf-8\")),\n",
        "    usecols=[\n",
        "        \"subject_ids\",\n",
        "        \"classification_id\",\n",
        "        \"workflow_id\",\n",
        "        \"workflow_version\",\n",
        "        \"annotations\",\n",
        "        \"created_at\",\n",
        "        \"user_name\",\n",
        "    ],\n",
        ")"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RuKdtJCQQkA"
      },
      "source": [
        "## Specify the video and frame workflows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgosFjI7QYJQ"
      },
      "source": [
        "workflow_clip = 11767\n",
        "workflow_clip_version = 227\n",
        "workflow_frame = 12852\n",
        "workflow_frame_version = 21.85 #Should this be 21.43?"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFBeHSwTPybJ"
      },
      "source": [
        "### Format video annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzgxh0HgPzRU"
      },
      "source": [
        "# Filter clip classifications\n",
        "class_clip = class_df[\n",
        "    (class_df.workflow_id >= workflow_clip)\n",
        "    & (class_df.workflow_version >= workflow_clip_version)\n",
        "].reset_index()\n",
        "\n",
        "# Create an empty list\n",
        "rows_list = []\n",
        "\n",
        "# Loop through each classification submitted by the users\n",
        "for index, row in class_clip.iterrows():\n",
        "    # Load annotations as json format\n",
        "    annotations = json.loads(row[\"annotations\"])\n",
        "\n",
        "    # Select the information from the species identification task\n",
        "    for ann_i in annotations:\n",
        "        if ann_i[\"task\"] == \"T4\":\n",
        "\n",
        "            # Select each species annotated and flatten the relevant answers\n",
        "            for value_i in ann_i[\"value\"]:\n",
        "                choice_i = {}\n",
        "                # If choice = 'nothing here', set follow-up answers to blank\n",
        "                if value_i[\"choice\"] == \"NOTHINGHERE\":\n",
        "                    f_time = \"\"\n",
        "                    inds = \"\"\n",
        "                # If choice = species, flatten follow-up answers\n",
        "                else:\n",
        "                    answers = value_i[\"answers\"]\n",
        "                    for k in answers.keys():\n",
        "                        if \"FIRSTTIME\" in k:\n",
        "                            f_time = answers[k].replace(\"S\", \"\")\n",
        "                        if \"INDIVIDUAL\" in k:\n",
        "                            inds = answers[k]\n",
        "\n",
        "                # Save the species of choice, class and subject id\n",
        "                choice_i.update(\n",
        "                    {\n",
        "                        \"classification_id\": row[\"classification_id\"],\n",
        "                        \"label\": value_i[\"choice\"],\n",
        "                        \"first_seen\": f_time,\n",
        "                        \"how_many\": inds,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                rows_list.append(choice_i)\n",
        "\n",
        "# Create a data frame with annotations as rows\n",
        "class_clips_df = pd.DataFrame(\n",
        "    rows_list, columns=[\"classification_id\", \"label\", \"first_seen\", \"how_many\"]\n",
        ")\n",
        "\n",
        "# Specify the type of columns of the df\n",
        "class_clips_df[\"how_many\"] = pd.to_numeric(class_clips_df[\"how_many\"])\n",
        "class_clips_df[\"first_seen\"] = pd.to_numeric(class_clips_df[\"first_seen\"])\n",
        "\n",
        "# Add subject id to each annotation\n",
        "class_clips_df = pd.merge(\n",
        "    class_clips_df,\n",
        "    class_clip.drop(columns=[\"annotations\"]),\n",
        "    how=\"left\",\n",
        "    on=\"classification_id\",\n",
        ")"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LdI_z19Q8Vx"
      },
      "source": [
        "## Format frame annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPfPZCVokkTF"
      },
      "source": [
        "# Filter frame classifications\n",
        "class_frame = class_df[\n",
        "    (class_df.workflow_id >= workflow_frame)\n",
        "    & (class_df.workflow_version >= workflow_frame_version)\n",
        "].reset_index()    \n",
        "\n",
        "# Create an empty list\n",
        "rows_list = []\n",
        "\n",
        "# Loop through each classification submitted by the users\n",
        "for index, row in class_frame.iterrows():\n",
        "    # Load annotations as json format\n",
        "    annotations = json.loads(row[\"annotations\"])\n",
        "\n",
        "    # Select the information from each annotation\n",
        "    for ann_i in annotations:\n",
        "      choice_i = {}\n",
        "\n",
        "      if not ann_i[\"value\"]:\n",
        "        # Save the annotation and class id\n",
        "        choice_i.update(\n",
        "            {\n",
        "                \"classification_id\": row[\"classification_id\"],\n",
        "                \"label\": \"no_coral\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "      else:\n",
        "        # Save the annotation and class id\n",
        "        choice_i.update(\n",
        "            {\n",
        "                \"classification_id\": row[\"classification_id\"],\n",
        "                \"label\": \"coral\",\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        \n",
        "      rows_list.append(choice_i)\n",
        "\n",
        "# Create a data frame with annotations as rows\n",
        "class_frame_df = pd.DataFrame(\n",
        "    rows_list, columns=[\"classification_id\", \"label\"]\n",
        ")\n",
        "\n",
        "# Add subject id to each annotation\n",
        "class_frame_df = pd.merge(\n",
        "    class_frame_df,\n",
        "    class_frame.drop(columns=[\"annotations\"]),\n",
        "    how=\"left\",\n",
        "    on=\"classification_id\",\n",
        ")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKZGrSv7SR0m"
      },
      "source": [
        "## Combine classifications and subject information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLsTdbJOSlTt"
      },
      "source": [
        "# Combine video and frame classifications\n",
        "annot_df = pd.merge(class_clips_df, class_frame_df, how=\"outer\")\n",
        "\n",
        "# Drop workflow and n_users columns\n",
        "annot_df = annot_df.drop(columns=[\"workflow_id\", \"workflow_version\"])\n",
        "\n",
        "# Rename the subject_id field\n",
        "annot_df = annot_df.rename(\n",
        "    columns={\"subject_ids\": \"subject_id\"}\n",
        ")\n",
        "\n",
        "# Add the subject information\n",
        "annot_df = pd.merge(\n",
        "    annot_df,\n",
        "    subjects,\n",
        "    how=\"left\",\n",
        "    on=\"subject_id\",\n",
        ")"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4ZriVp7Iuv"
      },
      "source": [
        "## Save classifications as csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_KGEDjmTJCp"
      },
      "source": [
        "annot_df.to_csv('annotations_data.csv')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha4BT45e7mb9"
      },
      "source": [
        "# Download Zooniverse comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh8tMckM7pjV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "ffb823f6-73a1-468d-cd8c-59220fa952fa"
      },
      "source": [
        "# Get comments from Zooniverse\n",
        "export = project.get_export('talk_comments')\n",
        "export = gzip.decompress(export.content)\n",
        "\n",
        "# Save the response as pandas data frame\n",
        "data = json.loads(export.decode('utf-8')[export.decode('utf-8').find('['):export.decode('utf-8').rfind(']')+1])\n",
        "comment_df = pd.DataFrame(data)[[\n",
        "        \"board_title\",\n",
        "        \"comment_body\",\n",
        "        \"comment_focus_id\",\n",
        "        \"comment_id\",\n",
        "        \"discussion_title\",\n",
        "        \"comment_created_at\",\n",
        "        \"comment_user_login\",\n",
        "    ]]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-ceaacd080e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get comments from Zooniverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'talk_comments'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mexport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save the response as pandas data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/panoptes_client/exportable.py\u001b[0m in \u001b[0;36mget_export\u001b[0;34m(self, export_type, generate, wait, wait_timeout)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexport_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTALK_EXPORT_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mmedia_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_requests'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mmedia_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CnhjqC_CN3z"
      },
      "source": [
        "## Combine comments and subject Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZJAOmBUCSf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "4ce82b6d-8e75-4f34-92de-9e4d0ca622f0"
      },
      "source": [
        "# Rename the subject_id field\n",
        "comment_df = comment_df.rename(\n",
        "    columns={\"comment_focus_id\": \"subject_id\"}\n",
        ")\n",
        "\n",
        "# Add the subject information\n",
        "comment_df = pd.merge(\n",
        "    comment_df,\n",
        "    subjects,\n",
        "    how=\"left\",\n",
        "    on=\"subject_id\",\n",
        ")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-fde5de400fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rename the subject_id field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m comment_df = comment_df.rename(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"comment_focus_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"subject_id\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'comment_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nB6XA3zpUV7"
      },
      "source": [
        "# Remove comments from the Zooniverse team (i.e. non-user comments)\n",
        "comment_df = comment_df.dropna(subset=['subject_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaGMNQ2A7ps0"
      },
      "source": [
        "# Download Zooniverse tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUP7Ppgip9lJ"
      },
      "source": [
        "# Get comments from Zooniverse\n",
        "export = project.get_export('talk_tags')\n",
        "export = gzip.decompress(export.content)\n",
        "\n",
        "# Save the response as pandas data frame\n",
        "data = json.loads(export.decode('utf-8')[export.decode('utf-8').find('['):export.decode('utf-8').rfind(']')+1])\n",
        "tag_df = pd.DataFrame(data)[[\"name\", \"comment_id\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIe_DwsMumgk"
      },
      "source": [
        "## Combine tags and comments information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG7q4VA2umgk"
      },
      "source": [
        "# Add the comments information\n",
        "comment_df = pd.merge(\n",
        "    comment_df,\n",
        "    tag_df,\n",
        "    how=\"left\",\n",
        "    on=\"comment_id\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OnWhwyryM_6"
      },
      "source": [
        "## Save comments as csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIbhUwhcumgk"
      },
      "source": [
        "comment_df.to_csv('comments_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9REo1aGL7yh"
      },
      "source": [
        "Find out the period when the clip and frame workflows were active \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEZikLBqLQMr"
      },
      "source": [
        "# Filter only for subjects that are frames\n",
        "annot_frames = annot_df[(annot_df.subject_type == \"frame\")]\n",
        "\n",
        "# Select the first frame annotation\n",
        "first_day = annot_frames['created_at'].min()\n",
        "\n",
        "# Date when the last frame was retired\n",
        "last_day = annot_frames['retired_at'].max()"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo7gnZBm-Hbc"
      },
      "source": [
        "# May 16-19 classifications (old subject set)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Wak9ES1t9-xS",
        "outputId": "f10dd241-7119-4c25-e463-1897c0f5d72a"
      },
      "source": [
        "class_df[(class_df.created_at < '2020-05-20') & (class_df.workflow_id >= 12852) & (class_df.workflow_version >= 21.43)]['created_at'].min()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-05-16 21:33:04 UTC'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "oOUwcgTW-FHr",
        "outputId": "20a9b06f-fd8a-4f9f-b4bc-8d4b3806ade7"
      },
      "source": [
        "class_df[(class_df.created_at < '2020-05-20') & (class_df.workflow_id >= 12852) & (class_df.workflow_version >= 21.43)]['created_at'].max()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-05-19 22:34:12 UTC'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntkt4ws4-LdS"
      },
      "source": [
        "# New subject set"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "RyQiwu9OMO5u",
        "outputId": "ff8d5ac5-ca65-4797-d19d-cff28234c79b"
      },
      "source": [
        "first_day"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-05-29 07:39:06 UTC'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "ureXXHFVN_Nd",
        "outputId": "86926dfd-954e-444d-96ee-1e69fbd50b09"
      },
      "source": [
        "last_day"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2020-06-01 15:08:30 UTC'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Kcc1Osut6O"
      },
      "source": [
        "# END"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}