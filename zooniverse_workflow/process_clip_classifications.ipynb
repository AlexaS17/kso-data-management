{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Zooniverse classifications of clips\n",
    "Script to retrieve clip classification data from Zooniverse and update the Koster lab database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download classifications from Zooniverse\n",
    "Download the most up-to-date classifications provided by Zooniverse users to the Koster lab project (#9747) using the [Python SDK for Panoptes!](https://github.com/zooniverse/panoptes-python-client).\n",
    "Note, only Zooniverse project collaborators can retrieve classifications from the Koster lab Zooniverse project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import operator\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from panoptes_client import Project, Panoptes\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify project-specific info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Zooniverse with your username and password\n",
    "Panoptes.connect(username='', password='')\n",
    "\n",
    "# Specify the project number of the koster lab\n",
    "project = Project(9747)\n",
    "\n",
    "# Specify the workflow of interest and its version\n",
    "workflow_1 = 11767\n",
    "workflow_1_version = 227\n",
    "\n",
    "# Specify the location to write the csv files                \n",
    "all_class = '../all_classifications.csv'\n",
    "w1_class = '../workflow1_classifications.csv'\n",
    "out_w1_class = '../flatten_class_w1.csv' \n",
    "agg_w1_class = '../agg_class_w1.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions to download the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, dstn):\n",
    "    request = requests.get(url, stream=True)\n",
    "    with open(dstn, 'wb') as dstn_f:\n",
    "        for chunk in request.iter_content(chunk_size=4096):\n",
    "            dstn_f.write(chunk)\n",
    "    return dstn\n",
    "\n",
    "\n",
    "def download_exports(projt, dstn_cl):\n",
    "    # replace path and filename strings for where you want the exports saved in the next two lines:\n",
    "    try:\n",
    "        meta_class = projt.describe_export('classifications')\n",
    "        generated = meta_class['media'][0]['updated_at'][0:19]\n",
    "        tdelta = (datetime.now() - datetime.strptime(generated, '%Y-%m-%dT%H:%M:%S')).total_seconds()\n",
    "        age = (300 + int(tdelta / 60))\n",
    "        print(str(datetime.now())[0:19] + '  Classifications export', age, ' hours old')\n",
    "        url_class = meta_class['media'][0]['src']\n",
    "        file_class = download_file(url_class, dstn_cl)\n",
    "        print(str(datetime.now())[0:19] + '  ' + file_class + ' downloaded')\n",
    "    except:\n",
    "        print(str(datetime.now())[0:19] + '  Classifications download did not complete')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def include_class(class_record):\n",
    "    #  define a function that returns True or False based on whether the argument record is to be included or not in\n",
    "    #  the output file based on the conditional clauses.\n",
    "    #  many other conditions could be set up to determine if a record is to be processed and the flattened data\n",
    "    #  written to the output file. Any or all of these conditional tests that are not needed can be deleted or\n",
    "    # commented out with '#' placed in front of the line(s)of code that are not required.\n",
    "\n",
    "    if int(class_record['workflow_id']) == workflow_1:\n",
    "        pass  # replace'!= 0000' with '== xxxx' where xxxx is the workflow to include.  This is also useful to\n",
    "        # exclude a specific workflow as well.\n",
    "    else:\n",
    "        return False\n",
    "    if float(class_record['workflow_version']) >= workflow_1_version:\n",
    "        pass  # replace '001.01' with first version of the workflow to include.\n",
    "    else:\n",
    "        return False\n",
    "    if '2100-00-10 00:00:00 UTC' >= class_record['created_at'] >= '2000-00-10 00:00:00 UTC':\n",
    "        pass  # replace earliest and latest created_at date and times to select records commenced in a\n",
    "        #  specific time period\n",
    "    else:\n",
    "        return False\n",
    "    # otherwise :\n",
    "    return True\n",
    "\n",
    "\n",
    "def slice_exports(dstn_cl, out_location_cl):\n",
    "    with open(out_location_cl, 'w', newline='') as file:\n",
    "        fieldnames = ['classification_id',\n",
    "                      'user_name', 'user_id',\n",
    "                      'user_ip', 'workflow_id',\n",
    "                      'workflow_name',\n",
    "                      'workflow_version',\n",
    "                      'created_at',\n",
    "                      'gold_standard',\n",
    "                      'expert',\n",
    "                      'metadata',\n",
    "                      'annotations',\n",
    "                      'subject_data',\n",
    "                      'subject_ids']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # this area for initializing counters, status lists and loading pick lists into memory:\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        #  open the zooniverse data file using dictreader\n",
    "        with open(dstn_cl) as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                i += 1\n",
    "                if include_class(row):\n",
    "                    j += 1\n",
    "                    # This set up the writer to match the field names above and the variable names of their values:\n",
    "                    writer.writerow({'classification_id': row['classification_id'],\n",
    "                                     'user_name': row['user_name'],\n",
    "                                     'user_id': row['user_id'],\n",
    "                                     'user_ip': row['user_ip'],\n",
    "                                     'workflow_id': row['workflow_id'],\n",
    "                                     'workflow_name': row['workflow_name'],\n",
    "                                     'workflow_version': row['workflow_version'],\n",
    "                                     'created_at': row['created_at'],\n",
    "                                     'gold_standard': row['gold_standard'],\n",
    "                                     'expert': row['expert'],\n",
    "                                     'metadata': row['metadata'],\n",
    "                                     'annotations': row['annotations'],\n",
    "                                     'subject_data': row['subject_data'],\n",
    "                                     'subject_ids': row['subject_ids']})\n",
    "\n",
    "    # This area prints some basic process info and status\n",
    "    print(str(datetime.now())[0:19] + '  Classification file:' +\n",
    "          ' ' + str(i) + ' lines read and inspected' + ' ' + str(j) + ' records selected and copied')\n",
    "\n",
    "    k = 0\n",
    "    m = 0\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the classifications from Zooniverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 12:15:07  Classifications export 2652  hours old\n",
      "2020-03-25 12:15:09  ../all_classifications.csv downloaded\n",
      "True\n",
      "2020-03-25 12:15:09  Classification file: 2031 lines read and inspected 1850 records selected and copied\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(download_exports(project, all_class))\n",
    "    print(slice_exports(all_class, w1_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregrate Zooniverse classifications\n",
    "Combine the classifications of multiple users and aggregrate them to have one classification per clip. For example, clip_001 contains \"lobster\" and \"sponge\" at second 1 and 6, respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions to aggregate the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include(class_record):\n",
    "    #  define a function that returns True or False based on whether the argument record is to be\n",
    "    #  included or not in the output file based on the conditional clauses.\n",
    "    #  many other conditions could be set up to determine if a record is to be processed and the\n",
    "    #  flattened data written to the output file (see flatten_class_frame for more options).\n",
    "\n",
    "    if int(class_record['workflow_id']) == workflow_1 :\n",
    "        pass  # this one selects the workflow to include.\n",
    "    else:\n",
    "        return False\n",
    "    if float(class_record['workflow_version']) >= workflow_1_version:\n",
    "        pass  # this one selects the first version of the workflow to include. Note the workflows\n",
    "        #  must be compatible with the structure (task numbers) choices, and questions (they could\n",
    "        #  differ in confusions, characteristics or other wording differences.)\n",
    "    else:\n",
    "        return False\n",
    "    # otherwise :\n",
    "    return True\n",
    "\n",
    "def empty(ques, resp):\n",
    "    blank = []\n",
    "    for q1 in range(0, len(ques)):\n",
    "        blank.append([0 for r1 in resp[q1]])\n",
    "    return blank\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up output file structure with desired fields\n",
    "The list of field names must include each field required in the output. The names, and order must be exactly the same here as in the writer statement near the end of the program. The names and order are arbitrary - your choice, as long as they are the same in both locations.\n",
    "Additional fields from the classification file can be added or removed as required.  The other flatten_class blocks could be added to this demo similarly as they are added to flatten-class_frame either the general utility blocks or any other blocks if the workflow has more that the one survey task in it. These blocks should be added before the first survey task immediately after \"for task in annotations'. As code blocks are added to flatten the annotations JSON, columns need to be added to contain each newly split out group of data. Add each one using the format \"' new_field_name',\" .  Similarly fields can be removed from both places to reduce the file size if the information is not needed for the current purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850 lines read and inspected. 1850 records processed and 2543 lines written.\n"
     ]
    }
   ],
   "source": [
    "with open(out_w1_class, 'w', newline='') as ou_file:\n",
    "    fieldnames = ['classification_id',\n",
    "                  'subject_ids',\n",
    "                  'created_at',\n",
    "                  'user_name',\n",
    "                  'user_ip',\n",
    "                  'single_ann_clip_choice',\n",
    "                  'single_ann_clip_how_many',\n",
    "                  'single_ann_clip_first_time',\n",
    "                  'single_ann_clip_trash'\n",
    "                  ]\n",
    "    writer = csv.DictWriter(ou_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # this area for initializing counters, status lists and loading pick lists into memory:\n",
    "    rc2 = 0\n",
    "    rc1 = 0\n",
    "    wc1 = 0\n",
    "     \n",
    "    #  create a dictionary with the same question and response labels as the project builder.\n",
    "    questions = ['INDIVIDUAL','FIRSTTIME','TYPEOFOBJECT']\n",
    "    responses = [['1','2','3','4'], ['0S','1S','2S','3S','4S','5S','6S','7S','8S','9S'], ['FISHING', 'MATERIALLITTER']]\n",
    "\n",
    "    \n",
    "    #  open the zooniverse data file using dictreader, and load the more complex json strings\n",
    "    #  as python objects using json.loads()\n",
    "    with open(w1_class) as class_file:\n",
    "        classifications = csv.DictReader(class_file)\n",
    "        for row in classifications:\n",
    "            rc2 += 1\n",
    "            # useful for debugging - set the number of record to process at a low number ~1000\n",
    "            if rc2 == 150000:  # one more than the last line of zooniverse file to read if not EOF\n",
    "                break\n",
    "            if include(row) is True:\n",
    "                rc1 += 1\n",
    "                annotations = json.loads(row['annotations'])\n",
    "\n",
    "                # reset field variables for the survey task for each new row\n",
    "                choice = ''\n",
    "                answer = ['' for q4 in questions]\n",
    "\n",
    "                for task in annotations:\n",
    "                    # If the workflow has additional tasks or you want to add other general utilities\n",
    "                    # blocks, put them here before the survey task, so the writer block will have the\n",
    "                    # all the data it needs prior to the end of the survey task block.\n",
    "\n",
    "                    # The survey task block:\n",
    "                    try:\n",
    "                        #  main survey task recognized by project specific task number - in this case 'T0'\n",
    "                        #  you need this to match your own project - it may be different!\n",
    "                        if task['task'] == 'T4':\n",
    "                            try:\n",
    "                                for species in task['value']:\n",
    "                                    choice = species['choice']\n",
    "                                    answer_vector = empty(questions, responses)\n",
    "                                    \n",
    "                        \n",
    "                                    for q in range(0, len(questions)):\n",
    "                                        try:\n",
    "                                            answer[q] = [val for key, val in species['answers'].items() if questions[q] in key]    \n",
    "                                        except KeyError:\n",
    "                                            continue\n",
    "                                            \n",
    "                                    # This sets up the writer to match the field names above and the\n",
    "                                    # variable names of their values. Note we write one line per\n",
    "                                    # subject_choices:\n",
    "                                    wc1 += 1\n",
    "                                    writer.writerow({'classification_id': row['classification_id'],\n",
    "                                                     'subject_ids': row['subject_ids'],\n",
    "                                                     'created_at': row['created_at'],\n",
    "                                                     'user_name': row['user_name'],\n",
    "                                                     'user_ip': row['user_ip'],\n",
    "                                                     'single_ann_clip_choice': choice,\n",
    "                                                     'single_ann_clip_how_many': ''.join(map(str, answer[0])),\n",
    "                                                     'single_ann_clip_first_time': ''.join(map(str, answer[1])).replace(\"S\",\"\"),\n",
    "                                                     'single_ann_clip_trash': ''.join(map(str, answer[2]))\n",
    "                                                     })\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "# This area prints some basic process info and status\n",
    "print(rc2, 'lines read and inspected.', rc1, 'records processed and', wc1, 'lines written.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregrate the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the csv as df to handle with pandas\n",
    "w1_data = pd.read_csv(out_w1_class)\n",
    "\n",
    "#Calculate the number of different classifications per subject \n",
    "w1_data[\"class_subject\"] = w1_data.groupby('subject_ids')['classification_id'].transform('nunique')\n",
    "\n",
    "#Select subjects with at least 4 different classifications\n",
    "w1_data = w1_data[w1_data.class_subject > 3]\n",
    "\n",
    "#Calculate the proportion of users that agreed on their classifications\n",
    "w1_data[\"class_n\"] = w1_data.groupby(['subject_ids','single_ann_clip_choice'])['classification_id'].transform('count')\n",
    "w1_data[\"class_prop\"] = w1_data.class_n/w1_data.class_subject\n",
    "\n",
    "#Select subjects where at least 80% of the users agree in their classification\n",
    "w1_data = w1_data[w1_data.class_prop > .8]\n",
    "\n",
    "#extract the median of the second where the animal/object is and the number of animals\n",
    "w1_data = w1_data.groupby(['subject_ids','single_ann_clip_choice'], as_index=False)\n",
    "w1_data = pd.DataFrame(w1_data[['single_ann_clip_how_many', 'single_ann_clip_first_time']].median())\n",
    "\n",
    "#save csv file of the classified subjects\n",
    "w1_data.to_csv (agg_w1_class, index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the koster lab database with aggregated classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
