{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Zooniverse classifications of frames\n",
    "Script to retrieve frame classification data from Zooniverse and update the Koster lab database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download classifications from Zooniverse\n",
    "To download the most up-to-date classifications provided by Zooniverse users we need to request the classifications from the Koster lab project (#9747) using the [Python SDK for Panoptes!](https://github.com/zooniverse/panoptes-python-client)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from panoptes_client import Project, Panoptes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Zooniverse with your username and password\n",
    "Panoptes.connect(username='', password='')\n",
    "\n",
    "# Specify the project number of the koster lab\n",
    "project = Project(9747)\n",
    "\n",
    "# Specify the workflow of interest\n",
    "workflow_2 = 12852\n",
    "\n",
    "# Specifiy the location to write the csv files\n",
    "dstn_class = '../all_classifications.csv'\n",
    "out_location_class = '../workflow2_classifications.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-28 17:02:58  Classifications export 1168  hours old\n",
      "2020-02-28 17:02:59  ../all_classifications.csv downloaded\n",
      "2020-02-28 17:02:59  Subject export 37482  hours old\n",
      "True\n",
      "2020-02-28 17:02:59  Classification file: 1675 lines read and inspected 13 records selected and copied\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def download_file(url, dstn):\n",
    "    request = requests.get(url, stream=True)\n",
    "    with open(dstn, 'wb') as dstn_f:\n",
    "        for chunk in request.iter_content(chunk_size=4096):\n",
    "            dstn_f.write(chunk)\n",
    "    return dstn\n",
    "\n",
    "\n",
    "def download_exports(projt, dstn_cl):\n",
    "    # replace path and filename strings for where you want the exports saved in the next two lines:\n",
    "    try:\n",
    "        meta_class = projt.describe_export('classifications')\n",
    "        generated = meta_class['media'][0]['updated_at'][0:19]\n",
    "        tdelta = (datetime.now() - datetime.strptime(generated, '%Y-%m-%dT%H:%M:%S')).total_seconds()\n",
    "        age = (300 + int(tdelta / 60))\n",
    "        print(str(datetime.now())[0:19] + '  Classifications export', age, ' hours old')\n",
    "        url_class = meta_class['media'][0]['src']\n",
    "        file_class = download_file(url_class, dstn_cl)\n",
    "        print(str(datetime.now())[0:19] + '  ' + file_class + ' downloaded')\n",
    "    except:\n",
    "        print(str(datetime.now())[0:19] + '  Classifications download did not complete')\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        meta_subj = projt.describe_export('subjects')\n",
    "        generated = meta_subj['media'][0]['updated_at'][0:19]\n",
    "        tdelta = (datetime.now() - datetime.strptime(generated, '%Y-%m-%dT%H:%M:%S')).total_seconds()\n",
    "        age = (300 + int(tdelta / 60))\n",
    "        print(str(datetime.now())[0:19] + '  Subject export', age, ' hours old')\n",
    "    except:\n",
    "        print(str(datetime.now())[0:19] + '  Subjects download did not complete')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def include_class(class_record):\n",
    "    #  define a function that returns True or False based on whether the argument record is to be included or not in\n",
    "    #  the output file based on the conditional clauses.\n",
    "    #  many other conditions could be set up to determine if a record is to be processed and the flattened data\n",
    "    #  written to the output file. Any or all of these conditional tests that are not needed can be deleted or\n",
    "    # commented out with '#' placed in front of the line(s)of code that are not required.\n",
    "\n",
    "    if int(class_record['workflow_id']) == workflow_2:\n",
    "        pass  # replace'!= 0000' with '== xxxx' where xxxx is the workflow to include.  This is also useful to\n",
    "        # exclude a specific workflow as well.\n",
    "    else:\n",
    "        return False\n",
    "    if float(class_record['workflow_version']) >= 001.01:\n",
    "        pass  # replace '001.01' with first version of the workflow to include.\n",
    "    else:\n",
    "        return False\n",
    "    if 100000000 >= int(class_record['subject_ids']) >= 0000:\n",
    "        pass  # replace upper and lower subject_ids to include only a specified range of subjects - This is\n",
    "        # a very useful slice since subjects are selected together and can still be aggregated.\n",
    "    else:\n",
    "        return False\n",
    "    if not class_record['gold_standard'] and not class_record['expert']:\n",
    "        pass  # this excludes gold standard and expert classifications - remove the \"not\" to select only\n",
    "        # the gold standard or expert classifications\n",
    "    else:\n",
    "        return False\n",
    "    if '2100-00-10 00:00:00 UTC' >= class_record['created_at'] >= '2000-00-10 00:00:00 UTC':\n",
    "        pass  # replace earliest and latest created_at date and times to select records commenced in a\n",
    "        #  specific time period\n",
    "    else:\n",
    "        return False\n",
    "    # otherwise :\n",
    "    return True\n",
    "\n",
    "\n",
    "def slice_exports(dstn_cl, out_location_cl):\n",
    "    with open(out_location_cl, 'w', newline='') as file:\n",
    "        fieldnames = ['classification_id',\n",
    "                      'user_name', 'user_id',\n",
    "                      'user_ip', 'workflow_id',\n",
    "                      'workflow_name',\n",
    "                      'workflow_version',\n",
    "                      'created_at',\n",
    "                      'gold_standard',\n",
    "                      'expert',\n",
    "                      'metadata',\n",
    "                      'annotations',\n",
    "                      'subject_data',\n",
    "                      'subject_ids']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # this area for initializing counters, status lists and loading pick lists into memory:\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        #  open the zooniverse data file using dictreader\n",
    "        with open(dstn_cl) as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                i += 1\n",
    "                if include_class(row):\n",
    "                    j += 1\n",
    "                    # This set up the writer to match the field names above and the variable names of their values:\n",
    "                    writer.writerow({'classification_id': row['classification_id'],\n",
    "                                     'user_name': row['user_name'],\n",
    "                                     'user_id': row['user_id'],\n",
    "                                     'user_ip': row['user_ip'],\n",
    "                                     'workflow_id': row['workflow_id'],\n",
    "                                     'workflow_name': row['workflow_name'],\n",
    "                                     'workflow_version': row['workflow_version'],\n",
    "                                     'created_at': row['created_at'],\n",
    "                                     'gold_standard': row['gold_standard'],\n",
    "                                     'expert': row['expert'],\n",
    "                                     'metadata': row['metadata'],\n",
    "                                     'annotations': row['annotations'],\n",
    "                                     'subject_data': row['subject_data'],\n",
    "                                     'subject_ids': row['subject_ids']})\n",
    "\n",
    "    # This area prints some basic process info and status\n",
    "    print(str(datetime.now())[0:19] + '  Classification file:' +\n",
    "          ' ' + str(i) + ' lines read and inspected' + ' ' + str(j) + ' records selected and copied')\n",
    "\n",
    "    k = 0\n",
    "    m = 0\n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(download_exports(project, dstn_class))\n",
    "    print(slice_exports(dstn_class, out_location_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregrate the classifications and update the koster sql database\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
